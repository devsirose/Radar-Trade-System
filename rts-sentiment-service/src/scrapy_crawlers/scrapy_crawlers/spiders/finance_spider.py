import scrapy
from datetime import datetime
from scrapy_crawlers.items import FinanceArticleItem

FINANCE_KEYWORDS = [
    "t√†i ch√≠nh", "ch·ª©ng kho√°n", "l√£i su·∫•t", "ng√¢n h√†ng", "l·∫°m ph√°t",
    "ƒë·∫ßu t∆∞", "th·ªã tr∆∞·ªùng", "USD", "VND", "v√†ng", "gi√° d·∫ßu",
    "tr√°i phi·∫øu", "c·ªï phi·∫øu", "t·ª∑ gi√°", "kinh t·∫ø", "doanh nghi·ªáp",
    "l·ª£i nhu·∫≠n", "doanh thu", "ph√°t h√†nh", "IPO", "ni√™m y·∫øt",
    "qu·ªπ ƒë·∫ßu t∆∞", "thanh kho·∫£n", "gi·∫£m ph√°t", "v·ªën h√≥a", "FED", "l√£i vay",
    "bitcoin", "ethereum", "crypto", "ti·ªÅn s·ªë", "ti·ªÅn ƒëi·ªán t·ª≠",
    "blockchain", "defi", "web3", "altcoin", "binance"
]

def contains_finance_keyword(title, content):
    combined = (title + " " + content).lower()
    return any(keyword in combined for keyword in FINANCE_KEYWORDS)


class FinanceSpider(scrapy.Spider):
    name = "finance_spider"
    allowed_domains = ["vnexpress.net", "cafef.vn", "baodautu.vn", "vietnamfinance.vn"]
    start_urls = [
        "https://vnexpress.net/kinh-doanh",
        "https://cafef.vn/thi-truong-chung-khoan.chn",
        "https://baodautu.vn/chung-khoan/",
        "https://vietnamfinance.vn/tai-chinh.htm"
    ]

    custom_settings = {
        "DEFAULT_REQUEST_HEADERS": {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        },
        "DOWNLOAD_DELAY": 1,
        "RETRY_TIMES": 3,
        "CONCURRENT_REQUESTS": 2
    }

    def parse(self, response):
        print(f"\nüîé ƒêang parse URL: {response.url}")

        # VNExpress
        if "vnexpress.net" in response.url:
            links = response.css("h3.title-news a[href]::attr(href)").getall()[:10]
            print(f"‚û°Ô∏è VNExpress: {len(links)} link")
            for link in links:
                yield response.follow(link, callback=self.parse_article, meta={"source": "VNExpress"})

        # CafeF
        elif "cafef.vn" in response.url:
            links = response.css("h3 a::attr(href)").getall()[:10]
            print(f"‚û°Ô∏è CafeF: {len(links)} link")
            for link in links:
                yield response.follow(link, callback=self.parse_article, meta={"source": "CafeF"})

        # B√°o ƒê·∫ßu T∆∞
        elif "baodautu.vn" in response.url:
            links = response.css("h3 a::attr(href)").getall()[:10]
            print(f"‚û°Ô∏è B√°o ƒê·∫ßu t∆∞: {len(links)} link")
            for link in links:
                yield response.follow(link, callback=self.parse_article, meta={"source": "B√°o ƒê·∫ßu T∆∞"})

        # VietnamFinance
        elif "vietnamfinance.vn" in response.url:
            links = response.css("h3 a::attr(href)").getall()[:10]
            print(f"‚û°Ô∏è VietnamFinance: {len(links)} link")
            for link in links:
                yield response.follow(link, callback=self.parse_article, meta={"source": "VietnamFinance"})

    def parse_article(self, response):
        title = response.css("title::text").get(default="").strip()
        content = " ".join([p.strip() for p in response.css("p::text").getall() if p.strip()])

        print(f"\nüì∞ ƒêang parse: {title[:80]}...")

        if contains_finance_keyword(title, content):
            item = FinanceArticleItem()
            item["source"] = response.meta["source"]
            item["title"] = title
            item["link"] = response.url
            item["content_full"] = content
            item["published_at"] = datetime.utcnow()
            print(f"‚úÖ [OK] L∆∞u b√†i: {title}")
            yield item
        else:
            print(f"‚ùå [B·ªé] Kh√¥ng c√≥ t·ª´ kh√≥a: {title}")
